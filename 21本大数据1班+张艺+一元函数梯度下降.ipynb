{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461620ec-17ba-447c-9be3-8c71f102a09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0:  tensor(0.0121, requires_grad=True) y0:  tensor(0.0001, grad_fn=<PowBackward0>) x0.grad:  tensor(0.0242)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x0 = torch.tensor(5.0, requires_grad=True)\n",
    "y0 = x0**2\n",
    "\n",
    "alpha = 0.1\n",
    "epsilon = 0.0001\n",
    "\n",
    "x_values = []\n",
    "y_values = []\n",
    "\n",
    "# 循环直到y的变化小于epsilon\n",
    "while True:\n",
    "    # 计算梯度\n",
    "    y0.backward()\n",
    "    y = y0.item()  # 保存y0在当前迭代的取值，用于判断迭代的停止条件\n",
    "    # 更新x0\n",
    "    with torch.no_grad():\n",
    "        x0 -= alpha * x0.grad  # x0 = x0-alpha * x0.grad 创建一个新的张量并赋值给x\n",
    "    x0.grad.zero_()\n",
    "    # 计算新的y0\n",
    "    y0 = x0**2\n",
    "    # 检查是否满足结束条件\n",
    "    if abs(y0.item() - y) < epsilon:\n",
    "        break\n",
    "\n",
    "y0.backward()\n",
    "print(\"x0: \", x0, \"y0: \", y0, \"x0.grad: \", x0.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ad445-cf5b-49c1-9b40-a097494242e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6223c72-1ac4-4ddf-93e3-0988b012bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1962f63f-e005-471c-bc28-93e947cb9dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = 0.24, f(x) = 4.499925759999999\n",
      "Iteration 100: x = 0.9999249570513495, f(x) = 0.10000002252746701\n",
      "Iteration 200: x = 0.9999999820510311, f(x) = 0.1000000000000013\n",
      "Iteration 300: x = 0.9999999999957063, f(x) = 0.1\n",
      "Iteration 400: x = 0.999999999999999, f(x) = 0.1\n",
      "Iteration 500: x = 0.9999999999999993, f(x) = 0.1\n",
      "Iteration 600: x = 0.9999999999999993, f(x) = 0.1\n",
      "Iteration 700: x = 0.9999999999999993, f(x) = 0.1\n",
      "Iteration 800: x = 0.9999999999999993, f(x) = 0.1\n",
      "Iteration 900: x = 0.9999999999999993, f(x) = 0.1\n",
      "Approximate minimum at x = 0.9999999999999993, f(x) = 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 定义函数\n",
    "def f(x):\n",
    "    return (x - 1)**2 * (x - 3)**2 + 0.1\n",
    "\n",
    "# 定义函数的导数\n",
    "def df(x):\n",
    "    return 4 * (x - 1) * (x - 3) * (x - 2)\n",
    "\n",
    "# 梯度下降算法\n",
    "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
    "    x = starting_point\n",
    "    for i in range(num_iterations):\n",
    "        grad = df(x)\n",
    "        x = x - learning_rate * grad\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: x = {x}, f(x) = {f(x)}\")\n",
    "    return x\n",
    "\n",
    "# 初始点，学习率和迭代次数\n",
    "starting_point = 0\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "# 运行梯度下降算法\n",
    "min_x = gradient_descent(starting_point, learning_rate, num_iterations)\n",
    "print(f\"Approximate minimum at x = {min_x}, f(x) = {f(min_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ea348-15b8-4bf0-9d6c-d22172d186f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8832b11-b699-43a9-9907-d13983040dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 900: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1000: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 1900: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2000: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 2900: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3000: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 3900: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4000: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 4900: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5000: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 5900: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6000: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 6900: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7000: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 7900: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8000: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 8900: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9000: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9100: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9200: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9300: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9400: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9500: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9600: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9700: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9800: x = [0. 0.], f(x) = 2.0\n",
      "Iteration 9900: x = [0. 0.], f(x) = 2.0\n",
      "Approximate minimum at x = 0.0, y = 0.0, f(x, y) = 2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义一个具有两个波谷的二元函数\n",
    "def f(x, y):\n",
    "    return (x - 1)**2 * (y - 1)**2 + (x + 1)**2 * (y + 1)**2\n",
    "\n",
    "# 计算函数的梯度\n",
    "def grad_f(x, y):\n",
    "    df_dx = 4 * (x - 1) * (y - 1)**2 + 4 * (x + 1) * (y + 1)**2\n",
    "    df_dy = 4 * (y - 1) * (x - 1)**2 + 4 * (y + 1) * (x + 1)**2\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# 梯度下降算法\n",
    "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
    "    x = np.array(starting_point)\n",
    "    for i in range(num_iterations):\n",
    "        grad = grad_f(x[0], x[1])\n",
    "        x = x - learning_rate * grad\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: x = {x}, f(x) = {f(x[0], x[1])}\")\n",
    "    return x\n",
    "\n",
    "# 初始点，学习率和迭代次数\n",
    "starting_point = [0, 0]\n",
    "learning_rate = 0.01\n",
    "num_iterations = 10000\n",
    "\n",
    "# 运行梯度下降算法\n",
    "min_point = gradient_descent(starting_point, learning_rate, num_iterations)\n",
    "print(f\"Approximate minimum at x = {min_point[0]}, y = {min_point[1]}, f(x, y) = {f(min_point[0], min_point[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4d1df-34ad-45c5-bddc-58fb66d1d14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
